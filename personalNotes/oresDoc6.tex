\documentclass[12pt,a4paper]{article}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[unicode=true,bookmarks=false,bookmarksopen=true]{hyperref}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}

\usepackage{listings}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}

\definecolor{pGreen}{rgb}{0.44, 0.71, 0}
\definecolor{nRed}{rgb}{0.74, 0, 0}

\title{ORES Custom Documentation VI}
%\author{Tom GÃ¼lenman}
\date{}
\begin{document}
\maketitle
\textit{Disclaimer: No guarantee for the correctness of information / explanations / sources is given.}\\
%
\section*{Goals}
\begin{enumerate}
\item Metrics List: Create Table as a general quickview \checkmark
\item Metrics: which combinations are particularly useful, which are nonsensical?
\begin{itemize}
\item Ask for documentation on IRC (\checkmark)
\item Logically exclude combinations?
\item Document outputs
\end{itemize}
\item Recent Changes filter classes: how are edits assigned to them?
\begin{itemize}
\item Also ask for documentation on IRC \checkmark
\item Which metrics are included in the process? \checkmark
\item How are the metrics (precision, recall, threshold) included in the associated API calls? What do the (GET?)-Requests look like?
\end{itemize}
\item Take a closer look at the Threshold Plot for Logistic Regression (\href{http://www.scikit-yb.org/en/latest/api/classifier/threshold.html}{Link})
\begin{itemize}
\item What is the meaning of the areas around the curves? \checkmark
\item What is queue rate exactly? \checkmark
\end{itemize}
\item Take a closer look at the Swagger API Documentation
\item \colorbox{red}{ !!! } Improve knowledge of ORES Docs and foremost the metrics
\end{enumerate}
%
%
%
\newpage
\section{Metrics List: Table}
\begin{tabular}{|l|l|c|}
\hline
\textbf{Metric} & \textbf{Quick Definition} & \textbf{Value}\\\hline \hline
accuracy & Portion of correctly predicted data & $\frac{\texttt{TP}+\texttt{TN}}{\texttt{Total}}$\\\hline
counts & Number of \textbf{F}\&\textbf{T}-labels and predictions&\\\hline
f1 & Harmonic mean of recall and precision & $2*\frac{\texttt{rec} * \texttt{prec}}{\texttt{rec}+\texttt{prec}}$\\\hline
filter\_rate & Portion of observations predicted & $1-\texttt{match\_rate} =$\\ &to be negative&$\frac{\texttt{TN}+\texttt{FN}}{\texttt{Total}}$\\\hline
fpr & Probability of a false alarm & $\frac{\texttt{FP}}{\texttt{FP} + \texttt{TN}}$\\\hline
match\_rate & Portion of observations predicted & $\frac{\texttt{TP}+\texttt{FP}}{\texttt{Total}}$\\ &to be positive&\\\hline
pr\_auc & Measure of classification performance &\\\hline
precision & Ability to find only relevant cases & $\frac{\texttt{TP}}{\texttt{TP} + \texttt{FP}}$\\\hline
rates & Proportion of \textbf{F}\&\textbf{T}-labels to the total&\\\hline
recall & Ability to find \textbf{all} relevant cases & $\frac{\texttt{TP}}{\texttt{TP} + \texttt{FN}}$\\\hline
roc\_auc & Measure of classification performance &\\\hline
!f1 & Negated f1 & $2*\frac{\texttt{!rec} * \texttt{!prec}}{\texttt{!rec}+\texttt{!prec}}$\\\hline
!precision & Negated precision & $\frac{\texttt{TN}}{\texttt{TN} + \texttt{FN}}$ \\\hline
!recall & Negated recall & $\frac{\texttt{TN}}{\texttt{TN} + \texttt{FP}}$\\\hline
\end{tabular}
%TODO metrics doc add Probability of ranking a random positive higher than a random negative to roc_auc
%
%
%
\section{Metrics combinations}
example: \url{https://ores.wikimedia.org/v3/scores/enwiki/?models=damaging&model_info=statistics.thresholds.true.%27maximum%20!precision%20@%20precision%20%3E=%200.9%27}
%TODO
%TODO also see https://www.mediawiki.org/wiki/ORES/Thresholds -> worked example
%
%
%
\section{Recent Changes Quality Prediction Filters}
The Recent Changes quality prediction filters are a helpful tool in varying the precision and recall of catching damaging edits. They can be applied on the Recent changes site (\href{https://en.wikipedia.org/wiki/Special:RecentChanges?hidebots=1&hidecategorization=1&hideWikibase=1&limit=50&days=7&urlversion=2}{Link}).\\
\\
\includegraphics[scale=0.85]{resources/6/RCFilters}\\
{\href{https://en.wikipedia.org/wiki/Special:ORESModels}{Wikipedia Source}\\
\\
To put those numbers into context: we can expect that, for example, the \textit{Likely have problems} filter will be right about $45.7\%$ of the time, classifying a contribution as damaging while catching $48.1\%$ of problem edits.\\
To better understand threshold ranges it's helpful to also take a look at the following graphic:\\
\includegraphics[scale=0.5]{resources/6/RC-quality-filters-diagram}\\
\href{https://upload.wikimedia.org/wikipedia/commons/e/e8/RC-quality-filters-diagram.png}{Wikimedia Source}\\
%TODO explanation
%TODO Highlighting? https://www.mediawiki.org/wiki/Help:New_filters_for_edit_review/Highlighting_function
%
%
%
\section{\href{http://www.scikit-yb.org/en/latest/api/classifier/threshold.html}{Discrimination Threshold Visualisation (Logistic Regression)}}
\includegraphics[scale=0.7]{resources/4/discriminationThresholdVisualization}\\
\subsection{Areas - or bands - around the curves}
The model will split the data multiple times, differently, into train and test sets and then run the trials. This ensures a certain amount of variability being visualized. Corresponding section on the site: \\

``\textit{The visualizer also accounts for variability in the model by running multiple trials with different train and test splits of the data. The variability is visualized using a band such that the curve is drawn as the median score of each trial and the band is from the 10th to 90th percentile.}''

\subsection{Queue rate}
``This metric describes the percentage of instances that must be reviewed.''\\
It can be helpful to think about the costs of reviewing whatever it is that must be reviewed in the context of business decision, where the ability to review is a limited resource and might be a factor in adjusting the threshold in order to find a favourable outcome.
\end{document}